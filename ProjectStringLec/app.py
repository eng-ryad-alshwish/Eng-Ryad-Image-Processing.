"""
ØªØ·Ø¨ÙŠÙ‚ ÙÙ„Ø§ØªØ± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù†Ù…Ø§Ø·
Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Øµ Ù„Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠ 

"""
import cv2
import streamlit as st
import numpy as np
from PIL import Image
import tempfile
import time
import io
import base64
import os
import time

# Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙØ­Ø© Streamlit
st.set_page_config(
    page_title="ğŸ¥ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…",
    page_icon="ğŸ¥",
    layout="wide"
)

# ØªØ­Ø¯ÙŠØ¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸
save_folder = "saved_images"
os.makedirs(save_folder, exist_ok=True)

def crop_center(frame, target_width, target_height):
    h, w = frame.shape[:2]

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„Ø¥Ø·Ø§Ø±
    target_ratio = target_width / target_height
    current_ratio = w / h

    if current_ratio > target_ratio:
        # Ø§Ù„ØµÙˆØ±Ø© Ø£Ø¹Ø±Ø¶ Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø± â†’ Ù†Ù‚ØªØµ Ù…Ù† Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠÙ†
        new_w = int(h * target_ratio)
        offset_w = (w - new_w) // 2
        cropped = frame[:, offset_w:offset_w + new_w]
    else:
        # Ø§Ù„ØµÙˆØ±Ø© Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø± â†’ Ù†Ù‚ØªØµ Ù…Ù† Ø§Ù„Ø£Ø¹Ù„Ù‰ ÙˆØ§Ù„Ø£Ø³ÙÙ„
        new_h = int(w / target_ratio)
        offset_h = (h - new_h) // 2
        cropped = frame[offset_h:offset_h + new_h, :]

    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ø¬ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø© Ù„ØªØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ø§Ù„Ø¥Ø·Ø§Ø±
    return cv2.resize(cropped, (target_width, target_height))

# ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙÙ„Ø§ØªØ±
def apply_grayscale(frame):
    return cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

def apply_gaussian_blur(frame, kernel_size=15):
    return cv2.GaussianBlur(frame, (kernel_size, kernel_size), 0)

def apply_edge_detection(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 100, 200)
    return cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)

def apply_sepia(frame):
    kernel = np.array([[0.272, 0.534, 0.131],
                       [0.349, 0.686, 0.168],
                       [0.393, 0.769, 0.189]])
    frame = cv2.transform(frame, kernel)
    return np.clip(frame, 0, 255).astype(np.uint8)

def apply_invert(frame):
    return cv2.bitwise_not(frame)

def apply_sketch(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    inv_gray = 255 - gray
    blur = cv2.GaussianBlur(inv_gray, (21, 21), 0)
    inv_blur = 255 - blur
    sketch = cv2.divide(gray, inv_blur, scale=256.0)
    return cv2.cvtColor(sketch, cv2.COLOR_GRAY2BGR)

def apply_emboss(frame):
    kernel = np.array([[-2, -1, 0],
                       [-1, 1, 1],
                       [0, 1, 2]])
    return cv2.filter2D(frame, -1, kernel)

def apply_lomo(frame):
    frame = cv2.convertScaleAbs(frame, alpha=1.3, beta=20)
    return cv2.applyColorMap(frame, cv2.COLORMAP_OCEAN)

def apply_vignette(frame):
    rows, cols = frame.shape[:2]
    kernel_x = cv2.getGaussianKernel(cols,200)
    kernel_y = cv2.getGaussianKernel(rows,200)
    kernel = kernel_y * kernel_x.T
    mask = 255 * kernel / np.linalg.norm(kernel)
    output = np.empty_like(frame)
    for i in range(3):
        output[:,:,i] = frame[:,:,i] * mask
    return output

def apply_warm(frame):
    return cv2.convertScaleAbs(frame, alpha=1.1, beta=30)

def apply_cool(frame):
    return cv2.applyColorMap(frame, cv2.COLORMAP_BONE)

def apply_contrast(frame):
    return cv2.convertScaleAbs(frame, alpha=2.0, beta=0)

def apply_cartoon(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    gray = cv2.medianBlur(gray, 7)
    edges = cv2.adaptiveThreshold(gray, 255,
                                  cv2.ADAPTIVE_THRESH_MEAN_C,
                                  cv2.THRESH_BINARY, 9, 2)
    color = cv2.bilateralFilter(frame, 9, 300, 300)
    return cv2.bitwise_and(color, color, mask=edges)

def apply_sepia_modern(frame):
    kernel = np.array([[0.272, 0.534, 0.131],
                       [0.349, 0.686, 0.168],
                       [0.393, 0.769, 0.189]])
    frame = cv2.transform(frame, kernel)
    return np.clip(frame,0,255)

def apply_hdr(frame):
    return cv2.detailEnhance(frame, sigma_s=10, sigma_r=0.15)

def apply_fun_warp(frame, amplitude=10, frequency=10):
    h, w = frame.shape[:2]
    x, y = np.meshgrid(np.arange(w), np.arange(h), indexing='xy')
    map_x = x + amplitude * np.sin(y / frequency)
    map_y = y + amplitude * np.sin(x / frequency)
    map_x = map_x.astype(np.float32)
    map_y = map_y.astype(np.float32)
    warped_frame = cv2.remap(frame, map_x, map_y, interpolation=cv2.INTER_LINEAR)
    return warped_frame

def apply_color_bubble(frame):
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    hsv[:,:,1] = cv2.add(hsv[:,:,1], 50)  # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ´Ø¨Ø¹
    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)

def apply_neon_glow(frame):
    blur = cv2.GaussianBlur(frame, (15,15), 0)
    return cv2.addWeighted(frame, 0.6, blur, 0.4, 0)

def apply_pixelate(frame, blocks=20):
    h, w = frame.shape[:2]
    temp = cv2.resize(frame, (blocks, blocks), interpolation=cv2.INTER_LINEAR)
    return cv2.resize(temp, (w,h), interpolation=cv2.INTER_NEAREST)

def apply_rainbow_invert(frame):
    inverted = cv2.bitwise_not(frame)
    rainbow = cv2.applyColorMap(inverted, cv2.COLORMAP_RAINBOW)
    return cv2.addWeighted(frame, 0.5, rainbow, 0.5, 0)


# ØªØ­Ù…ÙŠÙ„ Haar Cascades
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_eye.xml")

# ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ø§Ù„Ù†Ø¸Ø§Ø±Ø© Ù…Ø¹ Ù‚Ù†Ø§Ø© Ø£Ù„ÙØ§
glasses_img = cv2.imread("C:\\Users\\USER\\Desktop\\projectt\\11.png", cv2.IMREAD_UNCHANGED)  # PNG Ù…Ø¹ alpha
if glasses_img is None:
    st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ±Ø© Ø§Ù„Ù†Ø¸Ø§Ø±Ø© 8.png Ø£Ùˆ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„Ù‡Ø§")

else:
    # Ø¥Ø°Ø§ Ù„Ù… ØªØ­ØªÙˆÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø¹Ù„Ù‰ Ù‚Ù†Ø§Ø© Ø£Ù„ÙØ§ØŒ Ø£Ø¶Ù Ù‚Ù†Ø§Ø© Ø£Ù„ÙØ§ ÙƒØ§Ù…Ù„Ø©
    if glasses_img.shape[2] == 3:
        b, g, r = cv2.split(glasses_img)
        alpha = np.ones(b.shape, dtype=b.dtype) * 255
        glasses_img = cv2.merge([b, g, r, alpha])


# ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ø§Ù„Ù‚Ø¨Ø¹Ø© Ù…Ø¹ Ù‚Ù†Ø§Ø© Ø£Ù„ÙØ§
hat_img = cv2.imread("C:\\Users\\USER\\Desktop\\projectt\\16.png", cv2.IMREAD_UNCHANGED)
if hat_img is None:
    st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ±Ø© Ø§Ù„Ù‚Ø¨Ø¹Ø© hat.png Ø£Ùˆ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„Ù‡Ø§")
else:
    if hat_img.shape[2] == 3:  # Ø¥Ø°Ø§ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¯ÙˆÙ† Alpha
        b, g, r = cv2.split(hat_img)
        alpha = np.ones(b.shape, dtype=b.dtype) * 255
        hat_img = cv2.merge([b, g, r, alpha])


# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
BACKGROUND_PATH = "C:\\Users\\USER\\Desktop\\projectt\\bg.png"
background_img = cv2.imread(BACKGROUND_PATH)


# ÙƒØ§Ø¦Ù† Ù„Ø·Ø±Ø­ Ø§Ù„Ø®Ù„ÙÙŠØ©
fgbg = cv2.createBackgroundSubtractorMOG2(history=200, varThreshold=50, detectShadows=True)

def apply_virtual_background(frame):
    global background_img, fgbg

    if background_img is None:
        return frame  # Ø¥Ø°Ø§ Ù…Ø§ ÙÙŠ Ø®Ù„ÙÙŠØ© Ø¨Ø¯ÙŠÙ„Ø© Ù†Ø±Ø¬Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©

    h, w, _ = frame.shape
    bg_resized = cv2.resize(background_img, (w, h))

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø§Ø³Ùƒ (Ø§Ù„Ø­Ø±ÙƒØ© Ø£Ù…Ø§Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§)
    fgmask = fgbg.apply(frame)

    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø§Ø³Ùƒ
    _, fgmask = cv2.threshold(fgmask, 200, 255, cv2.THRESH_BINARY)
    fgmask = cv2.medianBlur(fgmask, 5)

    # Ø§Ù„Ù…Ø§Ø³Ùƒ Ø§Ù„Ø¹ÙƒØ³ÙŠ Ù„Ù„Ø®Ù„ÙÙŠØ©
    inv_mask = cv2.bitwise_not(fgmask)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø®Øµ
    person = cv2.bitwise_and(frame, frame, mask=fgmask)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©
    new_bg = cv2.bitwise_and(bg_resized, bg_resized, mask=inv_mask)

    # Ø¯Ù…Ø¬ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†
    output = cv2.add(person, new_bg)

    return output


# ÙÙ„ØªØ± Ù‚Ø¨Ø¹Ø© Ø§Ù„ØªØ®Ø±Ø¬
def apply_hat(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)

    for (x, y, w, h) in faces:
        # Ø§Ø¬Ø¹Ù„ Ø¹Ø±Ø¶ Ø§Ù„Ù‚Ø¨Ø¹Ø© Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„ÙˆØ¬Ù‡ Ø¨Ù†Ø³Ø¨Ø© 20%
        hat_width = int(w * 1.3)
        hat_height = int(hat_width * hat_img.shape[0] / hat_img.shape[1])

        # ØªÙ…Ø±ÙƒØ² Ø§Ù„Ù‚Ø¨Ø¹Ø©
        x_offset = x - int((hat_width - w) / 2)  # ØªÙˆØ³ÙŠØ· Ø§Ù„Ù‚Ø¨Ø¹Ø©
        y_offset = y - hat_height + int(h / 8)   # ÙÙˆÙ‚ Ø§Ù„Ø±Ø£Ø³ Ø¨Ù‚Ù„ÙŠÙ„

        # Ù‚Øµ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù„Ùˆ Ø®Ø±Ø¬Øª
        if y_offset < 0:
            hat_height += y_offset
            y_offset = 0
        if x_offset < 0:
            hat_width += x_offset
            x_offset = 0

        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ù‚Ø¨Ø¹Ø©
        hat_resized = cv2.resize(hat_img, (hat_width, hat_height), interpolation=cv2.INTER_AREA)

        # Ù‚Ù†Ø§Ø© Ø£Ù„ÙØ§
        if hat_resized.shape[2] == 4:
            alpha = hat_resized[:, :, 3] / 255.0
            rgb_hat = hat_resized[:, :, :3]

            y1, y2 = y_offset, y_offset + hat_height
            x1, x2 = x_offset, x_offset + hat_width

            # ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¨Ù‚Ø§Ø¡ Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙˆØ±Ø©
            if y2 > frame.shape[0]:
                y2 = frame.shape[0]
                rgb_hat = rgb_hat[:y2 - y1]
                alpha = alpha[:y2 - y1]
            if x2 > frame.shape[1]:
                x2 = frame.shape[1]
                rgb_hat = rgb_hat[:, :x2 - x1]
                alpha = alpha[:, :x2 - x1]

            # Ø¯Ù…Ø¬ Ø§Ù„Ù‚Ø¨Ø¹Ø© Ù…Ø¹ Ø§Ù„ØµÙˆØ±Ø©
            for c in range(3):
                frame[y1:y2, x1:x2, c] = (alpha * rgb_hat[:, :, c] +
                                          (1 - alpha) * frame[y1:y2, x1:x2, c])
    return frame

# ÙÙ„ØªØ± Ø§Ù„Ù†Ø¸Ø§Ø±Ø©
def apply_glasses(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)

    for (x, y, w, h) in faces:
        # Ø¶Ø¨Ø· Ø­Ø¬Ù… Ø§Ù„Ù†Ø¸Ø§Ø±Ø© Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„ÙˆØ¬Ù‡
        glasses_width = w
        glasses_height = int(glasses_width * glasses_img.shape[0] / glasses_img.shape[1])
        y_offset = y + int(h / 4)
        x_offset = x

        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø­Ø¬Ù…
        glasses_resized = cv2.resize(glasses_img, (glasses_width, glasses_height), interpolation=cv2.INTER_AREA)

        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù‚Ù†Ø§Ø© Ø£Ù„ÙØ§
        if glasses_resized.shape[2] == 4:
            alpha = glasses_resized[:, :, 3] / 255.0  # Ù‚Ù†Ø§Ø© Ø§Ù„Ø´ÙØ§ÙÙŠØ©
            rgb_glasses = glasses_resized[:, :, :3]

            y1, y2 = y_offset, y_offset + glasses_height
            x1, x2 = x_offset, x_offset + glasses_width

            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø¥Ø·Ø§Ø±
            if y2 > frame.shape[0]:
                y2 = frame.shape[0]
                rgb_glasses = rgb_glasses[:y2 - y1]
                alpha = alpha[:y2 - y1]
            if x2 > frame.shape[1]:
                x2 = frame.shape[1]
                rgb_glasses = rgb_glasses[:, :x2 - x1]
                alpha = alpha[:, :x2 - x1]

            # Ø¯Ù…Ø¬ Ø§Ù„Ù†Ø¸Ø§Ø±Ø© Ù…Ø¹ Ø§Ù„Ø¥Ø·Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ù†Ø§Ø© Ø£Ù„ÙØ§
            for c in range(3):
                frame[y1:y2, x1:x2, c] = (alpha * rgb_glasses[:, :, c] +
                                          (1 - alpha) * frame[y1:y2, x1:x2, c])
    return frame

# Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
with st.sidebar:
    st.title("Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙÙ„Ø§ØªØ± ÙˆØ§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")

    # Sliders Ù„ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø¹Ø±Ø¶ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
    display_width = st.slider("Ø¹Ø±Ø¶ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ", 320, 1280, 380)
    display_height = st.slider("Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ", 240, 960, 640)

    filter_options = {
        "Ø¨Ø¯ÙˆÙ† ÙÙ„ØªØ±": None,
        "Ù‚Ø¨Ø¹Ø© ğŸ©": apply_hat,
        "ØªØ¯Ø±Ø¬ Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ": apply_grayscale,
        "ØªÙ…ÙˆÙŠÙ‡ Gaussian": apply_gaussian_blur,
        "ÙƒØ´Ù Ø§Ù„Ø­ÙˆØ§Ù": apply_edge_detection,
        "Ù„ÙˆÙ† Ø³ÙŠØ¨ÙŠØ§": apply_sepia,
        "Ø¹ÙƒØ³ Ø§Ù„Ø£Ù„ÙˆØ§Ù†": apply_invert,
        "Ø±Ø³Ù… Ù‚Ù„Ù… Ø±ØµØ§Øµ": apply_sketch,
        "Ù†Ù‚Ø´ Ø¨Ø§Ø±Ø²": apply_emboss,
        "Ù„ÙˆÙ…Ùˆ / Ø±ÙŠØªØ±Ùˆ": apply_lomo,
        "ØªØ¸Ù„ÙŠÙ„ Ø§Ù„Ø£Ø·Ø±Ø§Ù (ÙÙŠÙ†ÙŠØª)": apply_vignette,
        "Ø£Ù„ÙˆØ§Ù† Ø¯Ø§ÙØ¦Ø©": apply_warm,
        "Ø£Ù„ÙˆØ§Ù† Ø¨Ø§Ø±Ø¯Ø©": apply_cool,
        "Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ†": apply_contrast,
        "ØªØ£Ø«ÙŠØ± ÙƒØ±ØªÙˆÙ†ÙŠ": apply_cartoon,
        "Ø³ÙŠØ¨ÙŠØ§ Ø­Ø¯ÙŠØ«": apply_sepia_modern,
        "ØªØ£Ø«ÙŠØ± HDR": apply_hdr,
        "ØªØ´ÙˆÙŠÙ‡ Ù…Ù…ØªØ¹": apply_fun_warp,
        "ÙÙ‚Ø§Ø¹Ø§Øª Ù…Ù„ÙˆÙ†Ø©": apply_color_bubble,
        "Ù†ÙŠÙˆÙ†": apply_neon_glow,
        "Ø¨ÙƒØ³Ù„Ø©": apply_pixelate,
        "Ù‚ÙˆØ³ Ù‚Ø²Ø­ Ø¹ÙƒØ³ÙŠ": apply_rainbow_invert,
        "Ù†Ø¸Ø§Ø±Ø©": apply_glasses,
        "Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø®Ù„ÙÙŠØ©": apply_virtual_background


    }

    selected_filter = st.radio("Ø§Ø®ØªØ± Ø§Ù„ÙÙ„ØªØ±:", list(filter_options.keys()))

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ + Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ÙŠÙ…Ù†
col_main, col_right = st.columns([6, 1])  # 3 Ù„Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØŒ 1 Ù„Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ÙŠÙ…Ù†
st.markdown("""
<style>
div[data-testid="column"]:nth-of-type(2) {
    position: sticky;
    top: 0;
    height: 100vh;
}
</style>
""", unsafe_allow_html=True)


# with col_right:
#     with st.expander("ğŸ“Œ Ø´Ø±ÙŠØ· Ø¬Ø§Ù†Ø¨ÙŠ Ø£ÙŠÙ…Ù†", expanded=True):
#         st.radio("Ø§Ø®ØªØ± ÙÙ„ØªØ±:", ["Ø¨Ø¯ÙˆÙ† ÙÙ„ØªØ±", "Grayscale", "Sepia"])
#         st.button("Ø§Ù„ØªÙ‚Ø§Ø· ØµÙˆØ±Ø© ğŸ“¸")
with col_right:
    st.markdown("**ğŸ“Œ Ø´Ø±ÙŠØ· Ø¬Ø§Ù†Ø¨ÙŠ Ø£ÙŠÙ…Ù†**")
    st.button("Ø§Ù„ØªÙ‚Ø§Ø· ØµÙˆØ±Ø© ğŸ“¸")

        # Ø²Ø± Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„ØµÙˆØ±Ø©
    if st.button("Ø§Ù„ØªÙ‚Ø§Ø· ØµÙˆØ±Ø© ğŸ“¸", use_container_width=True):
        st.session_state.capture_next = True

    # Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù„ØªÙ‚Ø·Ø©
    if st.session_state.get('captured_image') is not None:
        st.image(st.session_state.captured_image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù„ØªÙ‚Ø·Ø©", use_container_width=True)
        
        # Ø²Ø± Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©
        if st.button("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©"):
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… ÙØ±ÙŠØ¯ Ù„ÙƒÙ„ ØµÙˆØ±Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„
            file_name = f"captured_{int(time.time())}.jpg"
            save_path = os.path.join(save_folder, file_name)

            # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ
            cv2.imwrite(save_path, cv2.cvtColor(st.session_state.captured_image, cv2.COLOR_RGB2BGR))
            st.success(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯: {save_path}")


with col_main:
    #st.markdown('<h3 style="text-align:center;color:#6366f1;">ğŸ¥ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ± Ø§Ù„Ø­ÙŠ</h3>', unsafe_allow_html=True)
    frame_placeholder = st.empty()  # placeholder Ù„Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø­ÙŠ


    # Placeholder Ù„Ù„ÙÙŠØ¯ÙŠÙˆ
    frame_placeholder = st.empty()

    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
    if 'cap' not in st.session_state:
        st.session_state.cap = cv2.VideoCapture(0)
        st.session_state.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 380)
        st.session_state.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 650)

    # Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ù„ØªÙ‚Ø§Ø·
    if 'capture_next' not in st.session_state:
        st.session_state.capture_next = False
        st.session_state.captured_image = None

    # Ø­Ù„Ù‚Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø­ÙŠ
    while True:
        ret, frame = st.session_state.cap.read()
        if not ret:
            st.error("Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")
            break

        frame = cv2.flip(frame, 1)  # Ø§Ù†Ø¹ÙƒØ§Ø³ Ø£ÙÙ‚ÙŠ Ù…Ø«Ù„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©

        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„ØªØ±
        if selected_filter != "Ø¨Ø¯ÙˆÙ† ÙÙ„ØªØ±":
            frame_filtered = filter_options[selected_filter](frame)
            if len(frame_filtered.shape) == 2:  # Ø¥Ø°Ø§ ÙƒØ§Ù† grayscale
                frame_filtered = cv2.cvtColor(frame_filtered, cv2.COLOR_GRAY2BGR)
        else:
            frame_filtered = frame

        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ø¬ÙŠÙ… Ø§Ù„ÙØ±ÙŠÙ… Ù„Ù„ØªØ­ÙƒÙ… Ø¨Ø§Ù„Ø¹Ø±Ø¶ ÙˆØ§Ù„Ø§Ø±ØªÙØ§Ø¹
        frame_resized = cv2.resize(frame_filtered, (display_width, display_height))
        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
        frame_pil = Image.fromarray(frame_rgb)

        # Ø¯Ø§Ø®Ù„ Ø­Ù„Ù‚Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø­ÙŠ
        frame_cropped_resized = crop_center(frame_filtered, display_width, display_height)

        frame_rgb = cv2.cvtColor(frame_cropped_resized, cv2.COLOR_BGR2RGB)
        frame_pil = Image.fromarray(frame_rgb)

        # ØªØ­ÙˆÙŠÙ„ Ù„Ù„ØµÙˆØ±Ø© Base64 ÙˆØ¹Ø±Ø¶Ù‡Ø§ Ù…Ø¹ Ø§Ù„Ø¥Ø·Ø§Ø±
        buffered = io.BytesIO()
        frame_pil.save(buffered, format="JPEG")
        img_str = base64.b64encode(buffered.getvalue()).decode()

        frame_placeholder.markdown(
            f"""
            <div style="
                display:flex; 
                justify-content:center; 
                border: 5px solid #6366f1; 
                padding: 10px; 
                border-radius: 15px; 
                width: fit-content;
                margin: auto;
            ">
                <img src="data:image/jpeg;base64,{img_str}" width="{display_width}" height="{display_height}" />
            </div>
            """,
            unsafe_allow_html=True
        )
        
        # Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„ØµÙˆØ±Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø±
        if st.session_state.capture_next:
            st.session_state.captured_image = frame_rgb
            st.session_state.capture_next = False

        # ØªØ£Ø®ÙŠØ± Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
        time.sleep(0.03)
